{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deakin University\n",
    "\n",
    "Unit: SIT720 Machine Learning\n",
    "\n",
    "Assessment 2: Group problem solving task\n",
    "\n",
    "*Group Members*\n",
    "#Student Name: Marlon Brando Camilo - Student ID: 217629846\n",
    "#Student Name: Angelito Villar  - Student ID: 218471268\n",
    "#Student Name: Thuc Nguyen - Student ID: 218536504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 32)\n",
      "(20, 32)\n",
      "(100, 31)\n",
      "(20, 31)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Task 1.1 Read the training and testing data. Print the number of features in the dataset.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trainingDataFrame = pd.read_csv('train_wbcd.csv')\n",
    "testingDataFrame = pd.read_csv('test_wbcd.csv')\n",
    "\n",
    "print(trainingDataFrame.shape)\n",
    "print(testingDataFrame.shape)\n",
    "\n",
    "trainingData = trainingDataFrame.drop(columns=['Patient_ID'])\n",
    "testingData = testingDataFrame.drop(columns=['Patient_ID'])\n",
    "\n",
    "print(trainingData.shape)\n",
    "print(testingData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    58\n",
      "1    42\n",
      "Name: Diagnosis, dtype: int64\n",
      "0    14\n",
      "1     6\n",
      "Name: Diagnosis, dtype: int64\n",
      "    Patient_ID Diagnosis      f1     f2      f3      f4       f5       f6  \\\n",
      "0       909410         B  14.020  15.66   89.59   606.5  0.07966  0.05581   \n",
      "1     84358402         M  20.290  14.34  135.10  1297.0  0.10030  0.13280   \n",
      "2      8912284         B  12.890  15.70   84.08   516.6  0.07818  0.09580   \n",
      "3     90317302         B  10.260  12.22   65.75   321.6  0.09996  0.07542   \n",
      "4       914102         B  13.160  20.54   84.06   538.7  0.07335  0.05275   \n",
      "5       924342         B   9.333  21.94   59.01   264.0  0.09240  0.05605   \n",
      "6      8911164         B  11.890  17.36   76.20   435.6  0.12250  0.07210   \n",
      "7       893548         B  13.050  13.84   82.71   530.6  0.08352  0.03735   \n",
      "8       867739         M  18.450  21.91  120.20  1075.0  0.09430  0.09709   \n",
      "9       857374         B  11.940  18.24   75.71   437.6  0.08261  0.04751   \n",
      "10      853201         M  17.570  15.05  115.00   955.1  0.09847  0.11570   \n",
      "11      857373         B  13.640  16.34   87.21   571.8  0.07685  0.06059   \n",
      "12       86561         B  13.850  17.21   88.44   588.7  0.08785  0.06136   \n",
      "13      899147         B  11.950  14.96   77.23   426.7  0.11580  0.12060   \n",
      "14      859464         B   9.465  21.01   60.11   269.4  0.10440  0.07773   \n",
      "15     8911800         B  13.590  17.84   86.24   572.3  0.07948  0.04052   \n",
      "16      871641         B  11.080  14.71   70.21   372.7  0.10060  0.05743   \n",
      "17      922577         B  10.320  16.35   65.31   324.9  0.09434  0.04994   \n",
      "18      897604         B  12.990  14.23   84.08   514.3  0.09462  0.09965   \n",
      "19    88119002         M  19.530  32.47  128.00  1223.0  0.08420  0.11300   \n",
      "20      925277         B  14.590  22.68   96.39   657.1  0.08473  0.13300   \n",
      "21     8810436         B  15.270  12.91   98.17   725.5  0.08182  0.06230   \n",
      "22      908489         M  13.980  19.62   91.12   599.5  0.10600  0.11330   \n",
      "23      873701         M  15.700  20.31  101.20   766.6  0.09597  0.08799   \n",
      "24      915940         B  14.580  13.66   94.29   658.8  0.09832  0.08918   \n",
      "25      869476         B  11.900  14.65   78.11   432.8  0.11520  0.12960   \n",
      "26      906539         B  11.570  19.04   74.20   409.7  0.08546  0.07722   \n",
      "27      862548         M  14.420  19.77   94.48   642.5  0.09752  0.11410   \n",
      "28      868999         B   9.738  11.97   61.24   288.5  0.09250  0.04102   \n",
      "29      859196         B   9.173  13.86   59.20   260.9  0.07721  0.08751   \n",
      "..         ...       ...     ...    ...     ...     ...      ...      ...   \n",
      "70      922576         B  13.620  23.23   87.19   573.2  0.09246  0.06747   \n",
      "71      858981         B   8.598  20.98   54.66   221.8  0.12430  0.08963   \n",
      "72      862717         M  13.610  24.98   88.05   582.7  0.09488  0.08511   \n",
      "73      864496         B   8.726  15.83   55.84   230.9  0.11500  0.08201   \n",
      "74      843786         M  12.450  15.70   82.57   477.1  0.12780  0.17000   \n",
      "75      911916         M  16.250  19.51  109.80   815.8  0.10260  0.18930   \n",
      "76     9010259         B  13.050  18.59   85.09   512.0  0.10820  0.13040   \n",
      "77      921092         B   7.729  25.49   47.98   178.8  0.08098  0.04878   \n",
      "78      894089         B  12.490  16.85   79.19   481.6  0.08511  0.03834   \n",
      "79      854268         M  14.250  21.72   93.63   633.0  0.09823  0.10980   \n",
      "80      844981         M  13.000  21.82   87.50   519.8  0.12730  0.19320   \n",
      "81      875099         B   9.720  18.22   60.73   288.1  0.06950  0.02344   \n",
      "82     9113846         B  12.270  29.97   77.42   465.4  0.07699  0.03398   \n",
      "83       86355         M  22.270  19.67  152.80  1509.0  0.13260  0.27680   \n",
      "84      923169         B   9.683  19.34   61.05   285.7  0.08491  0.05030   \n",
      "85      855625         M  19.070  24.81  128.30  1104.0  0.09081  0.21900   \n",
      "86      912600         B  15.730  11.28  102.80   747.2  0.10430  0.12990   \n",
      "87      859575         M  18.940  21.31  123.60  1130.0  0.09009  0.10290   \n",
      "88    86730502         M  16.160  21.54  106.20   809.8  0.10080  0.12840   \n",
      "89      909445         M  17.270  25.42  112.40   928.8  0.08331  0.11090   \n",
      "90      855138         M  13.480  20.82   88.40   559.2  0.10160  0.12550   \n",
      "91        8670         M  15.460  19.48  101.70   748.9  0.10920  0.12230   \n",
      "92      925236         B   9.423  27.88   59.26   271.3  0.08123  0.04971   \n",
      "93      901288         M  20.640  17.35  134.80  1335.0  0.09446  0.10760   \n",
      "94      854002         M  19.270  26.47  127.90  1162.0  0.09401  0.17190   \n",
      "95     8611555         M  25.220  24.91  171.50  1878.0  0.10630  0.26650   \n",
      "96      873593         M  21.090  26.57  142.70  1311.0  0.11410  0.28320   \n",
      "97      891703         B  11.850  17.46   75.54   432.7  0.08372  0.05642   \n",
      "98      925311         B  11.200  29.37   70.67   386.0  0.07449  0.03558   \n",
      "99       89813         B  14.420  16.54   94.15   641.2  0.09751  0.11390   \n",
      "\n",
      "          f7        f8   ...        f21    f22     f23     f24      f25  \\\n",
      "0   0.020870  0.026520   ...     14.910  19.31   96.53   688.9  0.10340   \n",
      "1   0.198000  0.104300   ...     22.540  16.67  152.20  1575.0  0.13740   \n",
      "2   0.111500  0.033900   ...     13.900  19.69   92.12   595.6  0.09926   \n",
      "3   0.019230  0.019680   ...     11.380  15.65   73.23   394.5  0.13430   \n",
      "4   0.018000  0.012560   ...     14.500  28.46   95.29   648.3  0.11180   \n",
      "5   0.039960  0.012820   ...      9.845  25.05   62.86   295.8  0.11030   \n",
      "6   0.059290  0.074040   ...     12.400  18.99   79.46   472.4  0.13590   \n",
      "7   0.004559  0.008829   ...     14.730  17.40   93.96   672.4  0.10160   \n",
      "8   0.115300  0.068470   ...     22.520  31.39  145.60  1590.0  0.14650   \n",
      "9   0.019720  0.013490   ...     13.100  21.33   83.67   527.2  0.11440   \n",
      "10  0.098750  0.079530   ...     20.010  19.52  134.90  1227.0  0.12550   \n",
      "11  0.018570  0.017230   ...     14.670  23.19   96.08   656.7  0.10890   \n",
      "12  0.014200  0.011410   ...     15.490  23.58  100.30   725.9  0.11570   \n",
      "13  0.011710  0.017870   ...     12.810  17.72   83.09   496.2  0.12930   \n",
      "14  0.021720  0.015040   ...     10.410  31.56   67.03   330.7  0.15480   \n",
      "15  0.019970  0.012380   ...     15.500  26.10   98.91   739.1  0.10500   \n",
      "16  0.023630  0.025830   ...     11.350  16.82   72.01   396.5  0.12160   \n",
      "17  0.010120  0.005495   ...     11.250  21.77   71.12   384.9  0.12850   \n",
      "18  0.037380  0.020980   ...     13.720  16.91   87.38   576.0  0.11420   \n",
      "19  0.114500  0.066370   ...     27.900  45.41  180.20  2477.0  0.14080   \n",
      "20  0.102900  0.037360   ...     15.480  27.27  105.90   733.5  0.10260   \n",
      "21  0.058920  0.031570   ...     17.380  15.92  113.70   932.7  0.12220   \n",
      "22  0.112600  0.064630   ...     17.040  30.80  113.90   869.3  0.16130   \n",
      "23  0.065930  0.051890   ...     20.110  32.82  129.30  1269.0  0.14140   \n",
      "24  0.082220  0.043490   ...     16.760  17.24  108.50   862.0  0.12230   \n",
      "25  0.037100  0.030030   ...     13.150  16.51   86.26   509.6  0.14240   \n",
      "26  0.054850  0.014280   ...     13.070  26.98   86.43   520.5  0.12490   \n",
      "27  0.093880  0.058390   ...     16.330  30.86  109.50   826.4  0.14310   \n",
      "28  0.000000  0.000000   ...     10.620  14.10   66.53   342.9  0.12340   \n",
      "29  0.059880  0.021800   ...     10.010  19.23   65.59   310.1  0.09836   \n",
      "..       ...       ...   ...        ...    ...     ...     ...      ...   \n",
      "70  0.029740  0.024430   ...        NaN  29.09   97.58   729.8  0.12160   \n",
      "71  0.030000  0.009259   ...      9.565  27.04   62.06   273.9  0.16390   \n",
      "72  0.086250  0.044890   ...     16.990  35.27  108.60   906.5  0.12650   \n",
      "73  0.041320  0.019240   ...      9.628  19.62   64.48   284.4  0.17240   \n",
      "74  0.157800  0.080890   ...        NaN  23.75  103.40   741.6  0.17910   \n",
      "75  0.223600  0.091940   ...     17.390  23.05  122.10   939.7  0.13770   \n",
      "76  0.096030  0.056030   ...     14.190  24.85   94.22   591.2  0.13430   \n",
      "77  0.000000  0.000000   ...      9.077  30.92   57.17   248.0  0.12560   \n",
      "78  0.004473  0.006423   ...     13.340  19.71   84.48   544.2  0.11040   \n",
      "79  0.131900  0.055980   ...     15.890  30.36  116.20   799.6  0.14460   \n",
      "80  0.185900  0.093530   ...     15.490  30.73  106.20   739.3  0.17030   \n",
      "81  0.000000  0.000000   ...      9.968  20.83   62.25   303.8  0.07117   \n",
      "82  0.000000  0.000000   ...     13.450  38.05   85.08   558.9  0.09422   \n",
      "83  0.426400  0.182300   ...     28.400  28.01  206.80  2360.0  0.17010   \n",
      "84  0.023370  0.009615   ...     10.930  25.59   69.10   364.2  0.11990   \n",
      "85  0.210700  0.099610   ...     24.090  33.17  177.40  1651.0  0.12470   \n",
      "86  0.119100  0.062110   ...     17.010  14.20  112.50   854.3  0.15410   \n",
      "87  0.108000  0.079510   ...     24.860  26.58  165.90  1866.0  0.11930   \n",
      "88  0.104300  0.056130   ...     19.470  31.68  129.70  1175.0  0.13950   \n",
      "89  0.120400  0.057360   ...     20.380  35.46  132.80  1284.0  0.14360   \n",
      "90  0.106300  0.054390   ...     15.530  26.02  107.30   740.4  0.16100   \n",
      "91  0.146600  0.080870   ...     19.260  26.00  124.90  1156.0  0.15460   \n",
      "92  0.000000  0.000000   ...     10.490  34.24   66.50   330.6  0.10730   \n",
      "93  0.152700  0.089410   ...     25.370  23.17  166.80  1946.0  0.15620   \n",
      "94  0.165700  0.075930   ...     24.150  30.90  161.40  1813.0  0.15090   \n",
      "95  0.333900  0.184500   ...     30.000  33.62  211.70  2562.0  0.15730   \n",
      "96  0.248700  0.149600   ...     26.680  33.48  176.50  2089.0  0.14910   \n",
      "97  0.026880  0.022800   ...     13.060  25.75   84.35   517.8  0.13690   \n",
      "98  0.000000  0.000000   ...     11.920  38.30   75.19   439.6  0.09267   \n",
      "99  0.080070  0.042230   ...     16.670  21.51  111.40   862.1  0.12940   \n",
      "\n",
      "        f26      f27      f28     f29      f30  \n",
      "0   0.10170  0.06260  0.08216  0.2136  0.06710  \n",
      "1   0.20500  0.40000  0.16250  0.2364  0.07678  \n",
      "2   0.23170  0.33440  0.10170  0.1999  0.07127  \n",
      "3   0.16500  0.08615  0.06696  0.2937  0.07722  \n",
      "4   0.16460  0.07698  0.04195  0.2687  0.07429  \n",
      "5   0.08298  0.07993  0.02564  0.2435  0.07393  \n",
      "6   0.08368  0.07153  0.08946  0.2220  0.06033  \n",
      "7   0.05847  0.01824  0.03532  0.2107  0.06580  \n",
      "8   0.22750  0.39650  0.13790  0.3109  0.07610  \n",
      "9   0.08906  0.09203  0.06296  0.2785  0.07408  \n",
      "10  0.28120  0.24890  0.14560  0.2756  0.07919  \n",
      "11  0.15820  0.10500  0.08586  0.2346  0.08025  \n",
      "12  0.13500  0.08115  0.05104  0.2364  0.07182  \n",
      "13  0.18850  0.03122  0.04766  0.3124  0.07590  \n",
      "14  0.16640  0.09412  0.06517  0.2878  0.09211  \n",
      "15  0.07622  0.10600  0.05185  0.2335  0.06263  \n",
      "16  0.08240  0.03938  0.04306  0.1902  0.07313  \n",
      "17  0.08842  0.04384  0.02381  0.2681  0.07399  \n",
      "18  0.19750  0.14500  0.05850  0.2432  0.10090  \n",
      "19  0.40970  0.39950  0.16250  0.2713  0.07568  \n",
      "20  0.31710  0.36620  0.11050  0.2258  0.08004  \n",
      "21  0.21860  0.29620  0.10350  0.2320  0.07474  \n",
      "22  0.35680  0.40690  0.18270  0.3179  0.10550  \n",
      "23  0.35470  0.29020  0.15410  0.3437  0.08631  \n",
      "24  0.19280  0.24920  0.09186  0.2626  0.07048  \n",
      "25  0.25170  0.09420  0.06042  0.2727  0.10360  \n",
      "26  0.19370  0.25600  0.06664  0.3035  0.08284  \n",
      "27  0.30260  0.31940  0.15650  0.2718  0.09353  \n",
      "28  0.07204  0.00000  0.00000  0.3105  0.08151  \n",
      "29  0.16780  0.13970  0.05087  0.3282  0.08490  \n",
      "..      ...      ...      ...     ...      ...  \n",
      "70  0.15170  0.10490  0.07174  0.2642  0.06953  \n",
      "71  0.16980  0.09001  0.02778  0.2972  0.07712  \n",
      "72  0.19430  0.31690  0.11840  0.2651  0.07397  \n",
      "73  0.23640  0.24560  0.10500  0.2926  0.10170  \n",
      "74  0.52490  0.53550  0.17410  0.3985  0.12440  \n",
      "75  0.44620  0.58970  0.17750  0.3318  0.09136  \n",
      "76  0.26580  0.25730  0.12580  0.3113  0.08317  \n",
      "77  0.08340  0.00000  0.00000  0.3058  0.09938  \n",
      "78  0.04953  0.01938  0.02784  0.1917  0.06174  \n",
      "79  0.42380  0.51860  0.14470  0.3591  0.10140  \n",
      "80  0.54010  0.53900  0.20600  0.4378  0.10720  \n",
      "81  0.02729  0.00000  0.00000  0.1909  0.06559  \n",
      "82  0.05213  0.00000  0.00000  0.2409  0.06743  \n",
      "83  0.69970  0.96080  0.29100  0.4055  0.09789  \n",
      "84  0.09546  0.09350  0.03846  0.2552  0.07920  \n",
      "85  0.74440  0.72420  0.24930  0.4670  0.10380  \n",
      "86  0.29790  0.40040  0.14520  0.2557  0.08181  \n",
      "87  0.23360  0.26870  0.17890  0.2551  0.06589  \n",
      "88  0.30550  0.29920  0.13120  0.3480  0.07619  \n",
      "89  0.41220  0.50360  0.17390  0.2500  0.07944  \n",
      "90  0.42250  0.50300  0.22580  0.2807  0.10710  \n",
      "91  0.23940  0.37910  0.15140  0.2837  0.08019  \n",
      "92  0.07158  0.00000  0.00000  0.2475  0.06969  \n",
      "93  0.30550  0.41590  0.21120  0.2689  0.07055  \n",
      "94  0.65900  0.60910  0.17850  0.3672  0.11230  \n",
      "95  0.60760  0.64760  0.28670  0.2355  0.10510  \n",
      "96  0.75840  0.67800  0.29030  0.4098  0.12840  \n",
      "97  0.17580  0.13160  0.09140  0.3101  0.07007  \n",
      "98  0.05494  0.00000  0.00000  0.1566  0.05905  \n",
      "99  0.33710  0.37550  0.14140  0.3053  0.08764  \n",
      "\n",
      "[100 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "#For the data label, print the total number of 1's and 0's in the training and testing data. \n",
    "#Comment on the class distribution. Is it balanced or unbalanced?\n",
    "\n",
    "trainingData.loc[trainingData['Diagnosis']=='B','Diagnosis']=0\n",
    "trainingData.loc[trainingData['Diagnosis']=='M','Diagnosis']=1\n",
    "\n",
    "testingData.loc[testingData['Diagnosis']=='B','Diagnosis']=0\n",
    "testingData.loc[testingData['Diagnosis']=='M','Diagnosis']=1\n",
    "\n",
    "#Printing the total number of 0's and 1's in both the training and testing data\n",
    "print(trainingData['Diagnosis'].value_counts())\n",
    "print(testingData['Diagnosis'].value_counts())\n",
    "\n",
    "B = 0\n",
    "M = 1\n",
    "print(trainingDataFrame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison of the sum of 1's and 0's we got on the result are almost close to each other that illustrates as balance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Diagnosis    0\n",
       "f1           0\n",
       "f2           0\n",
       "f3           0\n",
       "f4           0\n",
       "f5           0\n",
       "f6           0\n",
       "f7           0\n",
       "f8           0\n",
       "f9           0\n",
       "f10          0\n",
       "f11          0\n",
       "f12          0\n",
       "f13          0\n",
       "f14          0\n",
       "f15          0\n",
       "f16          0\n",
       "f17          0\n",
       "f18          0\n",
       "f19          0\n",
       "f20          0\n",
       "f21          0\n",
       "f22          0\n",
       "f23          0\n",
       "f24          0\n",
       "f25          0\n",
       "f26          0\n",
       "f27          0\n",
       "f28          0\n",
       "f29          0\n",
       "f30          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Priniting the features of the testing data with missing entries\n",
    "trainingData.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Diagnosis    0\n",
       "f1           0\n",
       "f2           0\n",
       "f3           0\n",
       "f4           0\n",
       "f5           0\n",
       "f6           0\n",
       "f7           0\n",
       "f8           0\n",
       "f9           0\n",
       "f10          0\n",
       "f11          0\n",
       "f12          0\n",
       "f13          0\n",
       "f14          0\n",
       "f15          0\n",
       "f16          0\n",
       "f17          0\n",
       "f18          0\n",
       "f19          0\n",
       "f20          0\n",
       "f21          0\n",
       "f22          0\n",
       "f23          0\n",
       "f24          0\n",
       "f25          0\n",
       "f26          0\n",
       "f27          0\n",
       "f28          0\n",
       "f29          0\n",
       "f30          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing the features of the training data with missing entries\n",
    "testingData.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.486765306122454\n",
      "15.574578947368423\n"
     ]
    }
   ],
   "source": [
    "#Fill the missing entries. \n",
    "#For filling any feature, you can use either mean or median value of the feature values from observed entries.\n",
    "\n",
    "trainingf21mean = trainingData['f21'].mean()\n",
    "testingf21mean = testingData['f21'].mean()\n",
    "print(trainingf21mean)\n",
    "print(testingf21mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The null entry on the dataset was filled using mean value. \n",
    "trainingData=trainingData.fillna(trainingf21mean)\n",
    "testingData=testingData.fillna(testingf21mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the training and testing data.\n",
    "trainingData_norm = preprocessing.normalize(trainingData)\n",
    "testingData_norm = preprocessing.normalize(testingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.2 Logistic Regression Train logistic regression models with L1 regularization and L2 regularization using alpha = 0.1 and lambda = 0.1. \n",
    "\n",
    "trainingPredictors = trainingData_norm\n",
    "testingPredictors = testingData_norm\n",
    "trainingResponse = trainingData['Diagnosis']\n",
    "testingResponse = testingData['Diagnosis']                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy is: 0.9\n",
      "Model Precision score is: 1.0\n",
      "Model Recall score is: 0.6666666666666666\n",
      "Model F1 score is: 0.8\n",
      "Confusion Maxtrix:\n",
      "[[14  0]\n",
      " [ 2  4]]\n"
     ]
    }
   ],
   "source": [
    "#L1 and L2 regularization using lambda value 0.1\n",
    "#Report accuracy, precision, recall, f1-score and print the confusion matrix.\n",
    "\n",
    "lambda_val = 0.1\n",
    "#Initialize the Logitic regression model with l2 penalty\n",
    "lr = LogisticRegression(C=1/lambda_val, penalty='l2')\n",
    "lr.fit(trainingPredictors, trainingResponse)\n",
    "y_predict = lr.predict(testingPredictors)\n",
    " \n",
    "#Evaluate our model\n",
    "model_acc = accuracy_score(testingResponse, y_predict)\n",
    "model_precision = precision_score(testingResponse, y_predict)\n",
    "model_recall = recall_score(testingResponse, y_predict)\n",
    "model_f1 = f1_score(testingResponse, y_predict)\n",
    "print(\"Model Accuracy is: {}\".format(model_acc))\n",
    "print(\"Model Precision score is: {}\".format(model_precision))\n",
    "print(\"Model Recall score is: {}\".format(model_recall))\n",
    "print(\"Model F1 score is: {}\".format(model_f1))\n",
    "print(\"Confusion Maxtrix:\")\n",
    "print(confusion_matrix(testingResponse, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy is: 0.95\n",
      "Model Precision score is: 1.0\n",
      "Model Recall score is: 0.8333333333333334\n",
      "Model F1 score is: 0.9090909090909091\n",
      "Confusion Maxtrix:\n",
      "[[14  0]\n",
      " [ 1  5]]\n"
     ]
    }
   ],
   "source": [
    "#L1 and L2 regularization using Alpha value 0.1\n",
    "#Report accuracy, precision, recall, f1-score and print the confusion matrix.\n",
    "#The best hyperparameter will be the one that gives maximum validation accuracy.\n",
    "\n",
    "alpha_val = 0.1\n",
    "#Initialize the Logitic regression model with l2 penalty\n",
    "lr = LogisticRegression(C=1/alpha_val, penalty='l1')\n",
    "lr.fit(trainingPredictors, trainingResponse)\n",
    "y_predict = lr.predict(testingPredictors)\n",
    " \n",
    "#Evaluate our model\n",
    "model_acc = accuracy_score(testingResponse, y_predict)\n",
    "model_precision = precision_score(testingResponse, y_predict)\n",
    "model_recall = recall_score(testingResponse, y_predict)\n",
    "model_f1 = f1_score(testingResponse, y_predict)\n",
    "print(\"Model Accuracy is: {}\".format(model_acc))\n",
    "print(\"Model Precision score is: {}\".format(model_precision))\n",
    "print(\"Model Recall score is: {}\".format(model_recall))\n",
    "print(\"Model F1 score is: {}\".format(model_f1))\n",
    "print(\"Confusion Maxtrix:\")\n",
    "print(confusion_matrix(testingResponse, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.3 Choosing the best hyper-parameter\n",
    "#For each value of hyperparameter, perform 100 random splits of training data into training and validation data.\n",
    "\n",
    "predictors = list(trainingData.columns[1:31].values)\n",
    "response = ['Diagnosis']\n",
    "\n",
    "def runLRmodel(trials, data, predictors, label, penalty_type, penalty_score):\n",
    "\n",
    "    model_acc     = 0\n",
    "    for i in range(0,trials):\n",
    "        Dtrain, Dtest = train_test_split(data, test_size=0.3,shuffle=True)\n",
    "        lr = LogisticRegression(C=1/penalty_score, penalty=penalty_type)\n",
    "        lr.fit(Dtrain[predictors], Dtrain[label])\n",
    "        y_predict = lr.predict(Dtest[predictors])\n",
    "        model_acc += accuracy_score(Dtest[label], y_predict)\n",
    "\n",
    "    model_acc /= trials\n",
    "\n",
    "    return np.round(model_acc, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: [0.94 0.95 0.94 0.95 0.95 0.94 0.94 0.93 0.92 0.91]\n",
      "Best Lambda: 0.003\n"
     ]
    }
   ],
   "source": [
    "#For L2 model, choose the best lambda value from the following set: {0.001, 0.003, 0.01, 0.03, 0.1,0.3,1,3,10,33}.\n",
    "#Find the average validation accuracy for each 100 train/validate pairs.\n",
    "#The best hyperparameter will be the one that gives maximum validation accuracy.\n",
    "\n",
    "lambda_vals = [.001,.003,.01,.03,.1,.3,1,3,10,33]\n",
    "l2_acc = np.zeros(len(lambda_vals))\n",
    "index = 0\n",
    "\n",
    "#L2 regularization\n",
    "for l in lambda_vals:\n",
    "    l2_acc[index] = runLRmodel(100,trainingData, predictors, 'Diagnosis', 'l2', np.float(l))\n",
    "    index += 1\n",
    "\n",
    "print(\"Acc: {}\".format(l2_acc))\n",
    "\n",
    "# penalty at which validation accuracy is maximum\n",
    "max_index_l2  = np.argmax(l2_acc)\n",
    "best_lambda = lambda_vals[max_index_l2]\n",
    "print(\"Best Lambda: {}\".format(best_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: [0.96 0.94 0.91 0.9  0.88 0.89 0.88 0.44 0.44 0.44 0.57]\n",
      "Best Alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "#For L1 model, choose the best alpha value from the following set: {0.1,1,3,10,33,100,333,1000, 3333, 10000, 33333}.\n",
    "#Find the average validation accuracy for each 100 train/validate pairs.\n",
    "\n",
    "alpha_vals = [.1,1,3,10,33,100,333,1000,3333,10000,33333]\n",
    "l1_acc = np.zeros(len(alpha_vals))\n",
    "index = 0\n",
    "#L1 regularization\n",
    "for l in alpha_vals:\n",
    "    l1_acc[index] = runLRmodel(100,trainingData, predictors, 'Diagnosis', 'l1', np.float(l))\n",
    "    index += 1\n",
    "\n",
    "print(\"Acc: {}\".format(l1_acc))\n",
    "\n",
    "# penalty at which validation accuracy is maximum\n",
    "max_index_l1  = np.argmax(l1_acc)\n",
    "best_alpha = alpha_vals[max_index_l1]\n",
    "print(\"Best Alpha: {}\".format(best_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy is: 0.95\n",
      "Model Precision score is: 1.0\n",
      "Model Recall score is: 0.8333333333333334\n",
      "Model F1 score is: 0.9090909090909091\n",
      "Confusion Maxtrix:\n",
      "[[14  0]\n",
      " [ 1  5]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Use the best alpha and lambda parameter to re-train your final L1 and L2 regularized model. \n",
    "#Evaluate the prediction performance on the test data and report the following:\n",
    "#• Precision\n",
    "#• Accuracy\n",
    "#• The top 5 features selected in decreasing order of feature weights.\n",
    "#• Confusion matrix\n",
    "\n",
    "lambda_val = best_lambda\n",
    "#Initialize the Logitic regression model with l2 penalty\n",
    "lr = LogisticRegression(C=1/lambda_val, penalty='l2')\n",
    "lr.fit(trainingPredictors, trainingResponse)\n",
    "y_predict = lr.predict(testingPredictors)\n",
    " \n",
    "#Evaluate our model\n",
    "model_acc = accuracy_score(testingResponse, y_predict)\n",
    "model_precision = precision_score(testingResponse, y_predict)\n",
    "model_recall = recall_score(testingResponse, y_predict)\n",
    "model_f1 = f1_score(testingResponse, y_predict)\n",
    "print(\"Model Accuracy is: {}\".format(model_acc))\n",
    "print(\"Model Precision score is: {}\".format(model_precision))\n",
    "print(\"Model Recall score is: {}\".format(model_recall))\n",
    "print(\"Model F1 score is: {}\".format(model_f1))\n",
    "print(\"Confusion Maxtrix:\")\n",
    "print(confusion_matrix(testingResponse, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy is: 0.95\n",
      "Model Precision score is: 1.0\n",
      "Model Recall score is: 0.8333333333333334\n",
      "Model F1 score is: 0.9090909090909091\n",
      "Confusion Maxtrix:\n",
      "[[14  0]\n",
      " [ 1  5]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Use the best alpha and lambda parameter to re-train your final L1 and L2 regularized model. \n",
    "#Evaluate the prediction performance on the test data and report the following:\n",
    "#• Precision\n",
    "#• Accuracy\n",
    "#• The top 5 features selected in decreasing order of feature weights.\n",
    "#• Confusion matrix\n",
    "\n",
    "alpha_val = best_alpha\n",
    "#Initialize the Logitic regression model with l2 penalty\n",
    "lr = LogisticRegression(C=1/alpha_val, penalty='l1')\n",
    "lr.fit(trainingPredictors, trainingResponse)\n",
    "y_predict = lr.predict(testingPredictors)\n",
    " \n",
    "#Evaluate our model\n",
    "model_acc = accuracy_score(testingResponse, y_predict)\n",
    "model_precision = precision_score(testingResponse, y_predict)\n",
    "model_recall = recall_score(testingResponse, y_predict)\n",
    "model_f1 = f1_score(testingResponse, y_predict)\n",
    "print(\"Model Accuracy is: {}\".format(model_acc))\n",
    "print(\"Model Precision score is: {}\".format(model_precision))\n",
    "print(\"Model Recall score is: {}\".format(model_recall))\n",
    "print(\"Model F1 score is: {}\".format(model_f1))\n",
    "print(\"Confusion Maxtrix:\")\n",
    "print(confusion_matrix(testingResponse, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting value illustrates overfitting data due to the approximate values are almost similar that might indicate as the machine is learning from the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2520, 785)\n"
     ]
    }
   ],
   "source": [
    "# Task B Multiclass Classification\n",
    "df = pd.read_csv('reduced_mnist.csv')\n",
    "print (df.shape)\n",
    "#print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 9 4 2 8 6 3 5 0 7]\n"
     ]
    }
   ],
   "source": [
    "# no of data point 2520\n",
    "# no of features 784\n",
    "print(df['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy is: 0.8373015873015873\n",
      "Model Precision score is: 0.8337469708121903\n",
      "Model Recall score is: 0.8331453921334498\n"
     ]
    }
   ],
   "source": [
    "predictors = list(df.columns[1:785].values)\n",
    "response = ['label']\n",
    "Dtrain, Dtest = train_test_split(df, test_size=0.3)\n",
    "\n",
    "alpha_val = 1\n",
    "lr = LogisticRegression(C=1/alpha_val, penalty='l1')\n",
    "lr.fit(Dtrain[predictors], Dtrain['label'])\n",
    "y_predict = lr.predict(Dtest[predictors])\n",
    "\n",
    "#Evaluate our model\n",
    "model_acc = accuracy_score(Dtest[response], y_predict)\n",
    "model_precision = precision_score(Dtest[response], y_predict,average='macro')\n",
    "model_recall = recall_score(Dtest[response], y_predict,average='macro')\n",
    "print(\"Model Accuracy is: {}\".format(model_acc))\n",
    "print(\"Model Precision score is: {}\".format(model_precision))\n",
    "print(\"Model Recall score is: {}\".format(model_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLRmodel(trials, data, predictors, label, penalty_type, penalty_score):\n",
    "\n",
    "    model_acc_test=0\n",
    "    model_acc_train=0\n",
    "    \n",
    "    for i in range(0,trials):\n",
    "        Dtrain, Dtest = train_test_split(data, test_size=0.3,shuffle=True)\n",
    "        lr = LogisticRegression(C=1/penalty_score, penalty=penalty_type)\n",
    "        lr.fit(Dtrain[predictors], Dtrain[label])\n",
    "        y_predict_train = lr.predict(Dtrain[predictors])\n",
    "        y_predict_test = lr.predict(Dtest[predictors])\n",
    "        model_acc_train += accuracy_score(Dtrain[label],y_predict_train)\n",
    "        model_acc_test += accuracy_score(Dtest[label],y_predict_test)\n",
    "        \n",
    "    model_acc_test /= trials\n",
    "    model_acc_train /= trials\n",
    "\n",
    "    return np.round(model_acc_test, decimals=2), np.round(model_acc_train, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc Test: [0.84 0.83 0.83 0.83 0.83 0.84 0.87 0.85 0.8  0.7  0.54]\n",
      "Acc Train: [1.   1.   1.   1.   1.   1.   0.96 0.92 0.84 0.74 0.56]\n",
      "Best Alpha Test: 333\n",
      "Best Alpha Train: 0.1\n"
     ]
    }
   ],
   "source": [
    "alpha_vals = [.1,1,3,10,33,100,333,1000,3333,10000,33333]\n",
    "l1_acc = np.zeros(len(alpha_vals))\n",
    "l1_acc_train = np.zeros(len(alpha_vals))\n",
    "index = 0\n",
    "#L1 regularization\n",
    "for l in alpha_vals:\n",
    "    l1_acc[index], l1_acc_train[index]  = runLRmodel(10,Dtrain, predictors, 'label', 'l1', np.float(l))\n",
    "    index += 1\n",
    "\n",
    "print(\"Acc Test: {}\".format(l1_acc))\n",
    "print(\"Acc Train: {}\".format(l1_acc_train))\n",
    "# penalty at which validation accuracy is maximum\n",
    "max_index_l1  = np.argmax(l1_acc)\n",
    "max_index_l1_train  = np.argmax(l1_acc_train)\n",
    "best_alpha = alpha_vals[max_index_l1]\n",
    "best_alpha_train = alpha_vals[max_index_l1_train]\n",
    "print(\"Best Alpha Test: {}\".format(best_alpha))\n",
    "print(\"Best Alpha Train: {}\".format(best_alpha_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the accuracy curve\n",
    "plt.plot(range(0,len(alpha_vals)), l1_acc_train, color='r', label='Training Accuracy')\n",
    "plt.plot(range(0,len(alpha_vals)), l1_acc, color='b', label='Validation Accuracy')\n",
    "#replace the x-axis labels with penalty values\n",
    "plt.xticks(range(0,len(alpha_vals)), alpha_vals, rotation='vertical')\n",
    "\n",
    "#Highlight the best values of alpha and lambda\n",
    "plt.plot((max_index_l1_train, max_index_l1_train), (0, l1_acc_train[max_index_l1_train]), ls='dotted', color='r')\n",
    "plt.plot((max_index_l1, max_index_l1), (0, l1_acc[max_index_l1]), ls='dotted', color='b')\n",
    "\n",
    "#Set the y-axis from 0 to 1.0\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0, 1.0])\n",
    "\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy shows overfitting from 0.1 to 100 regions as this is impossible to meet a hundred percent accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy is: 0.8690476190476191\n",
      "Model Precision score is: 0.8695123602218077\n",
      "Model Recall score is: 0.8665720760823099\n",
      "Model Coeff: [0. 0. 0. ... 0. 0. 0.]\n",
      "(10, 784)\n",
      "965\n",
      "Confusion Maxtrix:\n",
      "[[74  1  0  0  0  0  2  0  1  0]\n",
      " [ 0 92  0  0  0  0  0  0  3  0]\n",
      " [ 2  1 56  0  0  0  0  1  3  1]\n",
      " [ 0  2  4 78  0  4  1  0  7  3]\n",
      " [ 1  0  1  0 59  0  2  0  1  5]\n",
      " [ 2  0  0  3  1 49  1  0  5  1]\n",
      " [ 2  1  2  0  1  0 72  0  1  0]\n",
      " [ 1  1  1  0  2  0  0 59  2  3]\n",
      " [ 0  5  2  0  0  4  0  0 59  0]\n",
      " [ 0  0  0  2  6  0  0  2  2 59]]\n"
     ]
    }
   ],
   "source": [
    "alpha_val = 333\n",
    "lr = LogisticRegression(C=1/alpha_val, penalty='l1')\n",
    "lr.fit(Dtrain[predictors], Dtrain['label'])\n",
    "y_predict = lr.predict(Dtest[predictors])\n",
    "\n",
    "#Evaluate our model\n",
    "model_acc = accuracy_score(Dtest[response], y_predict)\n",
    "model_precision = precision_score(Dtest[response], y_predict,average='macro')\n",
    "model_recall = recall_score(Dtest[response], y_predict,average='macro')\n",
    "print(\"Model Accuracy is: {}\".format(model_acc))\n",
    "print(\"Model Precision score is: {}\".format(model_precision))\n",
    "print(\"Model Recall score is: {}\".format(model_recall))\n",
    "print(\"Model Coeff: {}\".format(np.append(lr.intercept_, lr.coef_)))\n",
    "#(lr.intercept_, lr.coef_)\n",
    "print(lr.coef_.shape)\n",
    "print(np.count_nonzero(np.append(lr.intercept_, lr.coef_)))\n",
    "print(\"Confusion Maxtrix:\")\n",
    "print(confusion_matrix(Dtest[response], y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result appears to be underfittings as it is not sufficiently fit to the original training values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
